{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before You Start\n",
    "\n",
    "I am not installing any packages in this framework, so make sure to create a Python environment (Conda preferred) which contains the required packages. This will boil down to:\n",
    "\n",
    "- numpy\n",
    "- [more_itertools](https://pypi.org/project/more-itertools/)\n",
    "- PyTorch (including torchtext)\n",
    "- [sentencepiece](https://github.com/google/sentencepiece)\n",
    "- [tqdm](https://tqdm.github.io/)\n",
    "\n",
    "If you want to run this book in Visual Studio Code, you additionally need to have Jupyter packages installed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "\n",
    "NOTEBOOK_NAME = 'transformer-enfr-pytorch'\n",
    "\n",
    "# Training device.\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# Tokenization configuration.\n",
    "SOURCE_VOCAB_SIZE = 32000\n",
    "TARGET_VOCAB_SIZE = 32000\n",
    "PAD = 0\n",
    "BOS = 1 # beginning of statement\n",
    "EOS = 2 # end of statement\n",
    "\n",
    "# Transformer parameters.\n",
    "FEEDFORWARD_DIM = 2048\n",
    "EMBEDDING_DIM = 512\n",
    "MAX_SENTENCE = 88\n",
    "\n",
    "# Batching parameters.\n",
    "BUCKETING_BOUNDARIES = [8, 16, 24, 32, 48, 64, 72, 80, 88, 96, 104, 112, 120, 128]\n",
    "BUCKETING_SIZES = [196, 196, 196, 196, 132, 132, 132, 100, 100, 100, 64, 32, 32, 32, 32]\n",
    "\n",
    "def configure_logging():\n",
    "    logging.basicConfig(\n",
    "        handlers=[logging.StreamHandler()],\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s %(levelname)s %(message)s'\n",
    "    )\n",
    "\n",
    "    return logging.getLogger()\n",
    "\n",
    "\n",
    "log = configure_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:18:42.136648Z",
     "iopub.status.busy": "2021-12-13T06:18:42.136385Z",
     "iopub.status.idle": "2021-12-13T06:18:50.262172Z",
     "shell.execute_reply": "2021-12-13T06:18:50.261348Z",
     "shell.execute_reply.started": "2021-12-13T06:18:42.136612Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rafid/miniconda3/envs/seq2seq/lib/python3.10/site-packages/torch/utils/data/datapipes/utils/common.py:24: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "/home/rafid/miniconda3/envs/seq2seq/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/selecting.py:54: UserWarning: Lambda function is not supported for pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\"Lambda function is not supported for pickle, please use \"\n",
      "/home/rafid/miniconda3/envs/seq2seq/lib/python3.10/site-packages/torch/utils/data/datapipes/iter/combining.py:180: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"David Gallo: This is Bill Lange. I'm Dave Gallo.\\n\", 'David Gallo: Voici Bill Lange. Je suis Dave Gallo.\\n')\n",
      "('Everything I do, and everything I do professionally -- my life -- has been shaped by seven years of work as a young man in Africa.\\n', \"Chaque chose que je fais, chaque chose que je fais professionnellement -- ma vie -- a été façonnée par sept ans de travail en Afrique quand j'étais un jeune.\\n\")\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import IWSLT2016\n",
    "\n",
    "TRAIN_DATA_CACHE = None\n",
    "TEST_DATA_CACHE = None\n",
    "\n",
    "SOURCE_LANG = 'en'\n",
    "TARGET_LANG = 'fr'\n",
    "\n",
    "def train_data_gen():\n",
    "    global TRAIN_DATA_CACHE\n",
    "    if TRAIN_DATA_CACHE is None:\n",
    "        TRAIN_DATA_CACHE = list(IWSLT2016(split='train', language_pair=(SOURCE_LANG, TARGET_LANG)))\n",
    "    def gen(dummy=None):\n",
    "        return iter(TRAIN_DATA_CACHE)\n",
    "    return gen\n",
    "\n",
    "def test_data_gen():\n",
    "    global TEST_DATA_CACHE\n",
    "    if TEST_DATA_CACHE is None:\n",
    "        TEST_DATA_CACHE = list(IWSLT2016(split='valid', language_pair=(SOURCE_LANG, TARGET_LANG)))\n",
    "    def gen(dummy=None):\n",
    "        return iter(TEST_DATA_CACHE)\n",
    "    return gen\n",
    "\n",
    "print(next(iter(train_data_gen()())))\n",
    "print(next(iter(test_data_gen()())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a SentencePiece Text Tokenizer\n",
    "\n",
    "More information about this can be found [here](https://github.com/google/sentencepiece)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:18:50.265247Z",
     "iopub.status.busy": "2021-12-13T06:18:50.26453Z",
     "iopub.status.idle": "2021-12-13T06:19:44.243326Z",
     "shell.execute_reply": "2021-12-13T06:19:44.241978Z",
     "shell.execute_reply.started": "2021-12-13T06:18:50.265208Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/iwslt2016-enfr-en-textdump.txt\n",
      "  input_format: \n",
      "  model_prefix: ./data/iwslt2016-enfr-en-32k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 32000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: ./data/iwslt2016-enfr-en-textdump.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 221426 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=21274987\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9583% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=69\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999583\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 221426 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 177462 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 221426\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 132606\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 132606 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=61759 obj=10.5469 num_tokens=275296 num_tokens/piece=4.45759\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=51657 obj=8.17403 num_tokens=275772 num_tokens/piece=5.33852\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=38741 obj=8.14185 num_tokens=292852 num_tokens/piece=7.55923\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=38726 obj=8.13248 num_tokens=292998 num_tokens/piece=7.56592\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=35200 obj=8.146 num_tokens=299933 num_tokens/piece=8.52082\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=35199 obj=8.14364 num_tokens=299972 num_tokens/piece=8.52217\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: ./data/iwslt2016-enfr-en-32k.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: ./data/iwslt2016-enfr-en-32k.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./data/iwslt2016-enfr-fr-textdump.txt\n",
      "  input_format: \n",
      "  model_prefix: ./data/iwslt2016-enfr-fr-32k\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 32000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: ./data/iwslt2016-enfr-fr-textdump.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 221426 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=23689463\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9531% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=87\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999531\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 221426 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 230377 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 221426\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 166055\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 166055 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=77999 obj=11.4366 num_tokens=367769 num_tokens/piece=4.71505\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=66745 obj=8.85902 num_tokens=368398 num_tokens/piece=5.51948\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=50052 obj=8.84656 num_tokens=388416 num_tokens/piece=7.76025\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=50031 obj=8.83595 num_tokens=388391 num_tokens/piece=7.76301\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=37523 obj=8.90876 num_tokens=419800 num_tokens/piece=11.1878\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=37523 obj=8.89423 num_tokens=419839 num_tokens/piece=11.1888\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=35200 obj=8.90918 num_tokens=426426 num_tokens/piece=12.1144\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=35200 obj=8.90587 num_tokens=426438 num_tokens/piece=12.1147\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: ./data/iwslt2016-enfr-fr-32k.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: ./data/iwslt2016-enfr-fr-32k.vocab\n"
     ]
    }
   ],
   "source": [
    "# Load IMDb dataset and split it into training and test sets.\n",
    "import os \n",
    "import itertools\n",
    "import sentencepiece as spm\n",
    "\n",
    "FILENAME_PREFIX = f'iwslt2016-{SOURCE_LANG}{TARGET_LANG}'\n",
    "\n",
    "def data_filepath(filename):\n",
    "    return os.path.join('./data', filename)\n",
    "\n",
    "def build_sp_model(train, test, lang, vocab_size):\n",
    "    prefix = data_filepath(f'{FILENAME_PREFIX}-{lang}-{vocab_size//1000}k')\n",
    "    if os.path.isfile(f'{prefix}.model') and os.path.isfile(f'{prefix}.vocab'):\n",
    "        # Model is already built; no need to do anything.\n",
    "        return\n",
    "\n",
    "    # Dump the training and test data to a text file.\n",
    "    filename = data_filepath(f'{FILENAME_PREFIX}-{lang}-textdump.txt')\n",
    "    if not os.path.isfile(filename):\n",
    "        # Generate a unified text file containing the training and test data.\n",
    "        with open(filename, 'w') as f:\n",
    "            for stat in itertools.chain(train, test):\n",
    "                f.write(stat.strip() + '\\n')\n",
    "\n",
    "    spm.SentencePieceTrainer.train(input=filename, model_prefix=prefix, vocab_size=SOURCE_VOCAB_SIZE)\n",
    "            \n",
    "build_sp_model(\n",
    "    map(lambda x: x[0], train_data_gen()(None)), # Use 0 for source statement.\n",
    "    map(lambda x: x[0], test_data_gen()(None)), # Use 0 for source statement.\n",
    "    SOURCE_LANG,\n",
    "    SOURCE_VOCAB_SIZE\n",
    ")\n",
    "\n",
    "build_sp_model(\n",
    "    map(lambda x: x[1], train_data_gen()(None)), # Use 1 for target statement\n",
    "    map(lambda x: x[1], test_data_gen()(None)),  # Use 1 for target statement\n",
    "    TARGET_LANG,\n",
    "    TARGET_VOCAB_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Test SP Model\n",
    "\n",
    "\n",
    "Let's now load and test the SP models we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:19:44.245768Z",
     "iopub.status.busy": "2021-12-13T06:19:44.245235Z",
     "iopub.status.idle": "2021-12-13T06:19:44.312361Z",
     "shell.execute_reply": "2021-12-13T06:19:44.311701Z",
     "shell.execute_reply.started": "2021-12-13T06:19:44.245726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:18:46,358 INFO Test en tokenization with 'Hello, World!':\n",
      "2023-02-11 17:18:46,359 INFO [4832, 3, 827, 0]\n",
      "2023-02-11 17:18:46,359 INFO Test fr tokenization with 'Bonjour le monde':\n",
      "2023-02-11 17:18:46,359 INFO [2932, 10, 81]\n"
     ]
    }
   ],
   "source": [
    "sp_source_model = spm.SentencePieceProcessor(model_file=data_filepath(\n",
    "    f'{FILENAME_PREFIX}-{SOURCE_LANG}-{SOURCE_VOCAB_SIZE//1000}k.model'))\n",
    "sp_target_model = spm.SentencePieceProcessor(model_file=data_filepath(\n",
    "    f'{FILENAME_PREFIX}-{TARGET_LANG}-{TARGET_VOCAB_SIZE//1000}k.model'))\n",
    "log.info(f\"Test {SOURCE_LANG} tokenization with 'Hello, World!':\")\n",
    "log.info(f\"{sp_source_model.encode('Hello, World!')}\")\n",
    "log.info(f\"Test {TARGET_LANG} tokenization with 'Bonjour le monde':\")\n",
    "log.info(f\"{sp_target_model.encode('Bonjour le monde')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "\n",
    "Before we can use the text in the transformer, we have to convert it batches of tensor arrays \n",
    "that the transformer can understand. In this section, we will define a bunch of preprocessors\n",
    "that will help us do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "First, we need to be able to use the SentencePiece tokenizers we defined above to convert text into a sequence of tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:19:44.336144Z",
     "iopub.status.busy": "2021-12-13T06:19:44.335638Z",
     "iopub.status.idle": "2021-12-13T06:19:44.342263Z",
     "shell.execute_reply": "2021-12-13T06:19:44.341539Z",
     "shell.execute_reply.started": "2021-12-13T06:19:44.336092Z"
    }
   },
   "outputs": [],
   "source": [
    "def Tokenize(source_model, target_model):\n",
    "    def tokenize(iterable):\n",
    "        for source, target in iterable:\n",
    "            yield np.array(source_model.encode(source)), np.array(target_model.encode(target))\n",
    "\n",
    "    return lambda obj: tokenize(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statement Boundary\n",
    "\n",
    "Additionally, to know the boundary of a statement, we define a data pre-processing layer for adding BOS and EOS to each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:19:44.357044Z",
     "iopub.status.busy": "2021-12-13T06:19:44.356376Z",
     "iopub.status.idle": "2021-12-13T06:19:44.364852Z",
     "shell.execute_reply": "2021-12-13T06:19:44.364136Z",
     "shell.execute_reply.started": "2021-12-13T06:19:44.357008Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def StatementBoundary():\n",
    "    def end_of_statement(iterable):\n",
    "        for source, target in iterable:\n",
    "            # Yield three fields:\n",
    "            # 1. The sentence we use as the input to the encoder.\n",
    "            # 2. The sentence we use as the ipnut to the decoder.\n",
    "            # 3. The sentence we use as the target for the decoder.\n",
    "            yield (\n",
    "                # Encoder source\n",
    "                np.concatenate((source, [EOS])),\n",
    "                # Decoder source\n",
    "                np.concatenate(([BOS], target)),\n",
    "                # Decoder target.\n",
    "                np.concatenate((target, [EOS]))\n",
    "            )\n",
    "\n",
    "    return lambda obj: end_of_statement(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucket the training data by length to improve training speed. Bucketing by length means trying to generate buckets with samples having similar length to avoid large padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:19:44.366914Z",
     "iopub.status.busy": "2021-12-13T06:19:44.366179Z",
     "iopub.status.idle": "2021-12-13T06:19:44.380339Z",
     "shell.execute_reply": "2021-12-13T06:19:44.379532Z",
     "shell.execute_reply.started": "2021-12-13T06:19:44.366879Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def BucketByLength(bucket_boundaries, batch_sizes):\n",
    "    def pad(iterable, padded_size):\n",
    "        for el in iterable:\n",
    "            enc_src, dec_src, dec_trg = el\n",
    "            if max(len(enc_src), len(dec_src), len(dec_trg)) > padded_size:\n",
    "                raise ValueError(f'Source or target sentence is longer than required padding size.')\n",
    "            yield (\n",
    "                np.pad(enc_src, (PAD, padded_size - len(enc_src))),\n",
    "                np.pad(dec_src, (PAD, padded_size - len(dec_src))),\n",
    "                np.pad(dec_trg, (PAD, padded_size - len(dec_trg))),\n",
    "            )\n",
    "\n",
    "    bucket_boundaries = bucket_boundaries + [math.inf]  # Max boundary is unlimited.\n",
    "    def gen(iterable):\n",
    "        buckets = [[] for _ in bucket_boundaries]\n",
    "        for el in iterable:\n",
    "            enc_src, dec_src, dec_trg = el\n",
    "            bucket_idx = min([\n",
    "                idx\n",
    "                for idx, boundary in enumerate(bucket_boundaries)\n",
    "                if max(len(enc_src), len(dec_src), len(dec_trg)) < boundary\n",
    "            ])\n",
    "            buckets[bucket_idx].append(el)\n",
    "            if len(buckets[bucket_idx]) == batch_sizes[bucket_idx]:\n",
    "                bucket_boundary = bucket_boundaries[bucket_idx]\n",
    "                if math.isinf(bucket_boundary):\n",
    "                    # Find the maximum possible sentence and consider it to be\n",
    "                    # the bucket size.\n",
    "                    bucket_boundary = max([\n",
    "                        max(len(es), len(ds), len(dt))\n",
    "                        for es, ds, dt in buckets[bucket_idx]\n",
    "                    ])\n",
    "                padded = list(pad(buckets[bucket_idx], bucket_boundary))\n",
    "                yield (\n",
    "                    np.array(list(map(lambda x: x[0], padded))), # encoder source\n",
    "                    np.array(list(map(lambda x: x[1], padded))), # decoder source\n",
    "                    np.array(list(map(lambda x: x[2], padded))), # decoder target\n",
    "                )\n",
    "                buckets[bucket_idx] = []\n",
    "        # TODO yield the remaining buckets.\n",
    "    return gen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Long Sentences\n",
    "\n",
    "We need to limit the input we send to our model, so we define a layer for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FilterByLength(max_length, min_length=0, length_keys=None):\n",
    "    def _length_fn(x, length_keys):\n",
    "        if length_keys is not None and isinstance(x, (list, tuple)):\n",
    "            return max(len(x[idx]) for idx in length_keys)\n",
    "        elif length_keys is not None and isinstance(x, dict):\n",
    "            return max(len(x[key]) for key in length_keys)\n",
    "        else:\n",
    "            return len(x)\n",
    "\n",
    "    def filtered(iter):\n",
    "        for el in iter:\n",
    "            el_len = _length_fn(el, length_keys)\n",
    "\n",
    "            # Checking boundaries.\n",
    "            if max_length is not None and (\n",
    "                el_len > max_length or\n",
    "                el_len < min_length\n",
    "            ):\n",
    "                continue\n",
    "            # Within bounds.\n",
    "            yield el\n",
    "    return filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle\n",
    "\n",
    "To improve the training, we randomize the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from more_itertools import chunked\n",
    "\n",
    "def Shuffle(queue_size, seed=None):\n",
    "    def shuffle(iterable, queue_size, seed=None):\n",
    "        rnd = random.Random(seed)\n",
    "\n",
    "        for chunk in chunked(iterable, queue_size):\n",
    "            chunk = list(chunk)\n",
    "            rnd.shuffle(chunk)\n",
    "            for item in chunk:\n",
    "                yield item\n",
    "\n",
    "\n",
    "    return lambda iterable: shuffle(iterable, queue_size, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, having defined the filters above, we need to be able to execute them in serial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Serial(*layers):\n",
    "    \"\"\"Combines data processing layers into a single serial layer.\"\"\"\n",
    "    def serial(iterable=None):\n",
    "        for layer in layers:\n",
    "            iterable = layer(iterable)\n",
    "        return iterable\n",
    "    return serial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it altogether\n",
    "\n",
    "Having defined all the required preprocessing layers, we can now use them for loading the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:19:44.382183Z",
     "iopub.status.busy": "2021-12-13T06:19:44.381485Z",
     "iopub.status.idle": "2021-12-13T06:19:44.494193Z",
     "shell.execute_reply": "2021-12-13T06:19:44.493516Z",
     "shell.execute_reply.started": "2021-12-13T06:19:44.382154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   59,    74, 11141, ...,     0,     0,     0],\n",
       "        [  363,    64,   148, ...,     0,     0,     0],\n",
       "        [  116,    24,  3128, ...,     2,     0,     0],\n",
       "        ...,\n",
       "        [ 9799,    56,   320, ...,     0,     0,     0],\n",
       "        [   98,    16,   106, ...,     0,     0,     0],\n",
       "        [   98,    16,   418, ...,     0,     0,     0]]),\n",
       " array([[    1,    74,   577, ...,     0,     0,     0],\n",
       "        [    1,  2638,    23, ...,     0,     0,     0],\n",
       "        [    1,   132,    38, ...,     5,     0,     0],\n",
       "        ...,\n",
       "        [    1, 11235,   160, ...,     0,     0,     0],\n",
       "        [    1,    45,     4, ...,     0,     0,     0],\n",
       "        [    1,   106,   520, ...,     0,     0,     0]]),\n",
       " array([[   74,   577,    16, ...,     0,     0,     0],\n",
       "        [ 2638,    23,   450, ...,     0,     0,     0],\n",
       "        [  132,    38,    44, ...,     2,     0,     0],\n",
       "        ...,\n",
       "        [11235,   160,   420, ...,     0,     0,     0],\n",
       "        [   45,     4,    18, ...,     0,     0,     0],\n",
       "        [  106,   520,    69, ...,     0,     0,     0]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader = Serial(\n",
    "    train_data_gen(),\n",
    "    Tokenize(sp_source_model, sp_target_model),\n",
    "    StatementBoundary(),\n",
    "    FilterByLength(MAX_SENTENCE, length_keys=[0, 1, 2]),\n",
    "    Shuffle(1024, seed=10),\n",
    "    BucketByLength(BUCKETING_BOUNDARIES, BUCKETING_SIZES),\n",
    ")\n",
    "\n",
    "sample_enc_source_batch, sample_dec_source_batch, sample_dec_target_batch = next(data_loader())\n",
    "sample_enc_source_batch, sample_dec_source_batch, sample_dec_target_batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also print the original text for comparison purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:19:44.49744Z",
     "iopub.status.busy": "2021-12-13T06:19:44.497239Z",
     "iopub.status.idle": "2021-12-13T06:19:44.525854Z",
     "shell.execute_reply": "2021-12-13T06:19:44.525147Z",
     "shell.execute_reply.started": "2021-12-13T06:19:44.497408Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had marginal profit -- I did. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Nous avions un bénéfice marginal - je l'ai fait. ⁇  ⁇  ⁇ \n",
      "Nous avions un bénéfice marginal - je l'ai fait. ⁇  ⁇  ⁇ \n",
      "\n",
      "Let me take you on a little tour. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "Permettez-moi de vous en donner un petit aperçu. ⁇  ⁇  ⁇  ⁇ \n",
      "Permettez-moi de vous en donner un petit aperçu. ⁇  ⁇  ⁇  ⁇ \n",
      "\n",
      "They are neither feral nor myopically self-absorbed. ⁇  ⁇ \n",
      "Ils ne sont ni des sauvages ni des myopes égocentriques. ⁇  ⁇ \n",
      "Ils ne sont ni des sauvages ni des myopes égocentriques. ⁇  ⁇ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "for enc_src, dec_src, dec_trg in islice(zip(sample_enc_source_batch, sample_dec_target_batch, sample_dec_source_batch), 3):\n",
    "    print(sp_source_model.decode(enc_src.tolist()))\n",
    "    print(sp_target_model.decode(dec_src.tolist()))\n",
    "    print(sp_target_model.decode(dec_trg.tolist()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "\n",
    "Now, we need to build PyTorch modules for:\n",
    "\n",
    "- **Positional encoding**: Since a transformer takes all tokens in parallel, we need a mechanism to know the relative positions of the tokens in the sequence.\n",
    "- **Embedding**: This is the usual embedding to convert a word into an n-dimensional vector.\n",
    "\n",
    "For more information, see Vaswani et al. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Embedding(maxlen, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('indices', torch.arange(0, maxlen))\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding(self.indices[:token_embedding.size(1)]))\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens) * math.sqrt(self.emb_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Now we can define our transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Transformer\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 source_vocab_size,\n",
    "                 target_vocab_size,\n",
    "                 max_len,\n",
    "                 nhead = 8,\n",
    "                 num_encoder_layers = 6,\n",
    "                 num_decoder_layers = 6,\n",
    "                 dim_feedforward = 2048,\n",
    "                 dropout = 0.1):\n",
    "        super().__init__()\n",
    "        self.source_token_emb = TokenEmbedding(source_vocab_size, emb_size)\n",
    "        self.target_token_emb = TokenEmbedding(target_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout, maxlen=max_len)\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout,\n",
    "                                       batch_first=True)\n",
    "        self.generator = nn.Linear(emb_size, target_vocab_size)\n",
    "\n",
    "    def __create_source_masks(self, source_batch):\n",
    "        source_seq_len = source_batch.shape[1]\n",
    "\n",
    "        \n",
    "        # TODO Bad practice to use a hard-coded device.\n",
    "        source_mask = torch.zeros((source_seq_len, source_seq_len), device=DEVICE).type(torch.bool)\n",
    "        source_padding_mask = (source_batch == PAD)\n",
    "\n",
    "        return source_mask, source_padding_mask\n",
    "\n",
    "    def __create_target_masks(self, target_batch):\n",
    "        target_seq_len = target_batch.shape[1]\n",
    "\n",
    "        # TODO Bad practice to use a hard-coded device.\n",
    "        target_mask = self.transformer.generate_square_subsequent_mask(target_seq_len).to(DEVICE)\n",
    "        target_padding_mask = (target_batch == PAD)\n",
    "\n",
    "        return target_mask, target_padding_mask\n",
    "\n",
    "    def forward(self, source_batch, target_batch):                \n",
    "        source_mask, source_padding_mask = self.__create_source_masks(source_batch)\n",
    "        target_mask, target_padding_mask = self.__create_target_masks(target_batch)\n",
    "\n",
    "        source_emb = self.positional_encoding(self.source_token_emb(source_batch))\n",
    "        target_emb = self.positional_encoding(self.target_token_emb(target_batch))\n",
    "\n",
    "        outs = self.transformer.forward(\n",
    "            source_emb, target_emb, # source and target\n",
    "            source_mask, target_mask, None, # source, target, and memory masks\n",
    "            source_padding_mask, target_padding_mask, source_padding_mask # source, target, and memory padding mask\n",
    "        )\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, source_batch):\n",
    "        source_mask, _ = self.__create_source_masks(source_batch)\n",
    "        source_encoded = self.positional_encoding(self.source_token_emb(source_batch))\n",
    "        return self.transformer.encoder(source_encoded, source_mask)\n",
    "\n",
    "    def decode(self, target_batch, memory):\n",
    "        target_mask, _ = self.__create_target_masks(target_batch)\n",
    "        target_encoded = self.positional_encoding(self.target_token_emb(target_batch))\n",
    "        return self.transformer.decoder(target_encoded, memory, target_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (source_token_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(32000, 512)\n",
       "  )\n",
       "  (target_token_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(32000, 512)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (pos_embedding): Embedding(88, 512)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (2): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (3): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (4): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (5): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (generator): Linear(in_features=512, out_features=32000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq = Seq2SeqTransformer(EMBEDDING_DIM, SOURCE_VOCAB_SIZE, TARGET_VOCAB_SIZE, max_len=MAX_SENTENCE)\n",
    "seq2seq.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 88, 32000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import tensor\n",
    "test_source_batch = torch.randint(0, SOURCE_VOCAB_SIZE, (10, MAX_SENTENCE), device=DEVICE)\n",
    "test_target_batch = torch.randint(0, TARGET_VOCAB_SIZE, (10, MAX_SENTENCE), device=DEVICE)\n",
    "seq2seq(test_source_batch, test_target_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output is of shape `(batch_size, seq_len, target_vocab_size)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the total number of parameters in our transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93369600"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in seq2seq.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Search-based Translation\n",
    "\n",
    "To be able to find good results using an auto-regressive model, [beam search](https://en.wikipedia.org/wiki/Beam_search) is typically used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def translate_sentence_beam(self, tokenized_source, sp_target_model, max_output_len, max_candidates=4):\n",
    "    class Node(object):\n",
    "        def __init__(self, word, previous_node, probability):\n",
    "            self.word = word\n",
    "            self.probability = probability\n",
    "            self.previous_node = previous_node\n",
    "            self.len = 1 if previous_node is None else previous_node.len + 1\n",
    "\n",
    "        def word_list(self):\n",
    "            if self.previous_node is None:\n",
    "                return [self.word]\n",
    "            else:\n",
    "                return self.previous_node.word_list() + [self.word]\n",
    "\n",
    "        def sentence(self):\n",
    "            return sp_target_model.decode(self.word_list())\n",
    "        \n",
    "        def total_probability(self):\n",
    "            if self.previous_node is None:\n",
    "                return self.probability\n",
    "            else:\n",
    "                return self.probability + self.previous_node.total_probability()\n",
    "\n",
    "    memory = self.encode(tokenized_source)\n",
    "\n",
    "    queue = [Node(BOS, None, 0.0)]\n",
    "    candidates = []\n",
    "    \n",
    "    while queue:\n",
    "        new_queue = []\n",
    "        \n",
    "        for node in queue:\n",
    "            out = self.decode(\n",
    "                tensor([node.word_list()]).to(DEVICE),\n",
    "                memory,\n",
    "            )\n",
    "            predictions = F.log_softmax(self.generator(out), dim=-1)\n",
    "            \n",
    "            best_guesses_idxs = predictions.argsort(dim=2).flip(2)[0, -1, :max_candidates]\n",
    "            best_guesses_probs = predictions[0, -1, best_guesses_idxs]\n",
    "            \n",
    "            # Convert to a Python list for easier manipulation.\n",
    "            best_guesses_idxs = best_guesses_idxs.cpu().detach().numpy().tolist()\n",
    "            best_guesses_probs = best_guesses_probs.cpu().detach().numpy().tolist()\n",
    "            \n",
    "            for word, prob in zip(best_guesses_idxs, best_guesses_probs):\n",
    "                new_queue.append(Node(\n",
    "                    word,\n",
    "                    node,\n",
    "                    prob,\n",
    "                ))\n",
    "        \n",
    "        queue = []\n",
    "        \n",
    "        for node in sorted(new_queue, key=lambda n: n.total_probability(), reverse=True):\n",
    "            if node.word == EOS or node.len >= max_output_len:\n",
    "                candidates.append(node)\n",
    "            else:\n",
    "                queue.append(node)\n",
    "                if len(queue) == max_candidates:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "        if len(candidates) == 2*max_candidates:\n",
    "            # If we have many candidates already, we break even though there are\n",
    "            # still some paths to try.\n",
    "            break        \n",
    "\n",
    "    best_candidate = sorted(candidates, key=lambda n: n.total_probability(), reverse=True)[0]\n",
    "    \n",
    "    return best_candidate.sentence()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-13T06:19:50.550517Z",
     "iopub.status.busy": "2021-12-13T06:19:50.550208Z",
     "iopub.status.idle": "2021-12-13T06:53:22.502141Z",
     "shell.execute_reply": "2021-12-13T06:53:22.501413Z",
     "shell.execute_reply.started": "2021-12-13T06:19:50.550479Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:20:20,898 INFO Couldn't find a saved model. [Errno 2] No such file or directory: './data/transformer-enfr-pytorch.state'\n",
      "2023-02-11 17:20:20,898 INFO Couldn't find a saved model. Starting from scratch.\n",
      "2023-02-11 17:20:31,149 INFO During training, we will test with the following two sentences:\n",
      "2023-02-11 17:20:31,150 INFO Source Statement: \n",
      "2023-02-11 17:20:31,150 INFO We had marginal profit -- I did. ⁇  ⁇  ⁇  ⁇  ⁇  ⁇  ⁇ \n",
      "2023-02-11 17:20:31,150 INFO Target Statement: \n",
      "2023-02-11 17:20:31,150 INFO Nous avions un bénéfice marginal - je l'ai fait. ⁇  ⁇  ⁇ \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc7c5d62e6340ab9b8c9f27351450a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:20:31,158 INFO Starting epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea9a917da764a48991bfc0aa8d944e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:21:11,972 INFO Saving training state...\n",
      "/tmp/ipykernel_151633/280025697.py:127: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755897462/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  tensor([sample_enc_source_batch[0]]).to(DEVICE),\n",
      "2023-02-11 17:21:12,939 INFO Sample translation at epoch 1 and index 251.\n",
      "2023-02-11 17:21:12,940 INFO Predicted Statement: C'est pas..\n",
      "2023-02-11 17:21:55,702 INFO Saving training state...\n",
      "2023-02-11 17:21:56,997 INFO Sample translation at epoch 1 and index 502.\n",
      "2023-02-11 17:21:56,997 INFO Predicted Statement: Et j'ai dit que j'ai dit, je pense que nous.\n",
      "2023-02-11 17:22:37,516 INFO Saving training state...\n",
      "2023-02-11 17:22:39,186 INFO Sample translation at epoch 1 and index 753.\n",
      "2023-02-11 17:22:39,186 INFO Predicted Statement: Et j'ai commencé à l'intérieur de nous..\n",
      "2023-02-11 17:23:19,814 INFO Saving training state...\n",
      "2023-02-11 17:23:21,139 INFO Sample translation at epoch 1 and index 1004.\n",
      "2023-02-11 17:23:21,140 INFO Predicted Statement: Nous avons commencé à dire que nous avons commencé à l'école..\n",
      "2023-02-11 17:24:03,396 INFO Saving training state...\n",
      "2023-02-11 17:24:04,677 INFO Sample translation at epoch 1 and index 1255.\n",
      "2023-02-11 17:24:04,677 INFO Predicted Statement: Nous avons commencé à faire des années..\n",
      "2023-02-11 17:24:05,364 INFO Epoch 1 finished. Total loss is 4.855914625693542. Current learning rate is 0.0001\n",
      "2023-02-11 17:24:05,365 INFO Starting epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1fa73765c34a0e9cf897ce46a925e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:24:47,443 INFO Saving training state...\n",
      "2023-02-11 17:24:48,678 INFO Sample translation at epoch 2 and index 247.\n",
      "2023-02-11 17:24:48,678 INFO Predicted Statement: Nous avons commencé à faire, j'ai été en train d'être.\n",
      "2023-02-11 17:25:29,295 INFO Saving training state...\n",
      "2023-02-11 17:25:30,577 INFO Sample translation at epoch 2 and index 498.\n",
      "2023-02-11 17:25:30,577 INFO Predicted Statement: Nous avons eu l'idée d'utiliser -- j'ai fait un.\n",
      "2023-02-11 17:26:09,130 INFO Saving training state...\n",
      "2023-02-11 17:26:10,441 INFO Sample translation at epoch 2 and index 749.\n",
      "2023-02-11 17:26:10,441 INFO Predicted Statement: Nous avons eu l'intérieur -- j'ai fait l'intérieur de l'intérieur de l'intérieur de.\n",
      "2023-02-11 17:26:49,194 INFO Saving training state...\n",
      "2023-02-11 17:26:50,653 INFO Sample translation at epoch 2 and index 1000.\n",
      "2023-02-11 17:26:50,654 INFO Predicted Statement: Nous avons eu l'avenir -- j'ai fait une série d'art..\n",
      "2023-02-11 17:27:29,582 INFO Saving training state...\n",
      "2023-02-11 17:27:31,169 INFO Sample translation at epoch 2 and index 1251.\n",
      "2023-02-11 17:27:31,169 INFO Predicted Statement: Nous avions eu le sexe -- j'ai fait une approche de l'intérieur de l'intérieur..\n",
      "2023-02-11 17:27:32,379 INFO Epoch 2 finished. Total loss is 3.5087951113062306. Current learning rate is 0.0001\n",
      "2023-02-11 17:27:32,380 INFO Starting epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fc4021eb5343f981a8e2bceec68aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:28:09,932 INFO Saving training state...\n",
      "2023-02-11 17:28:11,317 INFO Sample translation at epoch 3 and index 243.\n",
      "2023-02-11 17:28:11,317 INFO Predicted Statement: Nous avons eu un plan -- j'ai fait un moyen d'utiliser l'intérieur..\n",
      "2023-02-11 17:28:48,576 INFO Saving training state...\n",
      "2023-02-11 17:28:49,837 INFO Sample translation at epoch 3 and index 494.\n",
      "2023-02-11 17:28:49,837 INFO Predicted Statement: Nous avons eu un outil d'action -- j'ai fait un .\n",
      "2023-02-11 17:29:29,244 INFO Saving training state...\n",
      "2023-02-11 17:29:30,494 INFO Sample translation at epoch 3 and index 745.\n",
      "2023-02-11 17:29:30,494 INFO Predicted Statement: Nous avions un moyen d'utiliser -- j'ai fait l'ensemble.\n",
      "2023-02-11 17:30:09,960 INFO Saving training state...\n",
      "2023-02-11 17:30:11,595 INFO Sample translation at epoch 3 and index 996.\n",
      "2023-02-11 17:30:11,595 INFO Predicted Statement: Nous avons eu un moyen de faire -- j'ai fait l'ensemble de l'environnement..\n",
      "2023-02-11 17:30:50,591 INFO Saving training state...\n",
      "2023-02-11 17:30:52,138 INFO Sample translation at epoch 3 and index 1247.\n",
      "2023-02-11 17:30:52,138 INFO Predicted Statement: Nous avons eu un moyen de créer -- j'ai fait une approche de l'intérieur..\n",
      "2023-02-11 17:30:53,907 INFO Epoch 3 finished. Total loss is 2.823320922915949. Current learning rate is 0.0001\n",
      "2023-02-11 17:30:53,908 INFO Starting epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eaf5d994269400b9e90443e878477ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:31:32,117 INFO Saving training state...\n",
      "2023-02-11 17:31:33,474 INFO Sample translation at epoch 4 and index 239.\n",
      "2023-02-11 17:31:33,474 INFO Predicted Statement: Nous avons eu un moyen de faire -- j'ai fait l'ensemble de l'objet de l'a.\n",
      "2023-02-11 17:32:12,033 INFO Saving training state...\n",
      "2023-02-11 17:32:13,288 INFO Sample translation at epoch 4 and index 490.\n",
      "2023-02-11 17:32:13,289 INFO Predicted Statement: Nous avons eu un bénéfice -- j'ai fait l'ensemble de l.\n",
      "2023-02-11 17:32:52,566 INFO Saving training state...\n",
      "2023-02-11 17:32:53,906 INFO Sample translation at epoch 4 and index 741.\n",
      "2023-02-11 17:32:53,906 INFO Predicted Statement: Nous avions un profit -- j'ai fait un oxydeage de l'ensemble de l'ensemble de l'ensemble..\n",
      "2023-02-11 17:33:30,901 INFO Saving training state...\n",
      "2023-02-11 17:33:32,515 INFO Sample translation at epoch 4 and index 992.\n",
      "2023-02-11 17:33:32,516 INFO Predicted Statement: Nous avons eu un profit, j'ai fait une part de l'ensemble de l'ensemble de l'.\n",
      "2023-02-11 17:34:10,868 INFO Saving training state...\n",
      "2023-02-11 17:34:12,521 INFO Sample translation at epoch 4 and index 1243.\n",
      "2023-02-11 17:34:12,521 INFO Predicted Statement: Nous avons eu un profit de profit, j'ai fait une part de l'œuvre d'un individu..\n",
      "2023-02-11 17:34:15,134 INFO Epoch 4 finished. Total loss is 2.3648890448146815. Current learning rate is 0.0001\n",
      "2023-02-11 17:34:15,135 INFO Starting epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90913877599c4e10a53a944ef22bd4f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:34:52,812 INFO Saving training state...\n",
      "2023-02-11 17:34:54,234 INFO Sample translation at epoch 5 and index 235.\n",
      "2023-02-11 17:34:54,234 INFO Predicted Statement: Nous avons eu le profit -- j'ai fait un impact sur l'.\n",
      "2023-02-11 17:35:32,765 INFO Saving training state...\n",
      "2023-02-11 17:35:34,201 INFO Sample translation at epoch 5 and index 486.\n",
      "2023-02-11 17:35:34,201 INFO Predicted Statement: Nous avons eu un bénéfice professionnel -- j'ai fait le point d'.\n",
      "2023-02-11 17:36:12,104 INFO Saving training state...\n",
      "2023-02-11 17:36:13,471 INFO Sample translation at epoch 5 and index 737.\n",
      "2023-02-11 17:36:13,472 INFO Predicted Statement: Nous avons eu un bénéfice -- j'ai fait un élément de l'ensemble de notre vie..\n",
      "2023-02-11 17:36:50,343 INFO Saving training state...\n",
      "2023-02-11 17:36:51,627 INFO Sample translation at epoch 5 and index 988.\n",
      "2023-02-11 17:36:51,628 INFO Predicted Statement: Nous avons eu un profit de profit -- J'ai fait de l'.\n",
      "2023-02-11 17:37:30,184 INFO Saving training state...\n",
      "2023-02-11 17:37:31,482 INFO Sample translation at epoch 5 and index 1239.\n",
      "2023-02-11 17:37:31,483 INFO Predicted Statement: Nous avons eu un profit de profit, j'ai fait de l'.\n",
      "2023-02-11 17:37:34,895 INFO Epoch 5 finished. Total loss is 2.0451185852880984. Current learning rate is 0.0001\n",
      "2023-02-11 17:37:34,896 INFO Starting epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6832fb868c27422ab4d8bcce451172b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:38:09,027 INFO Saving training state...\n",
      "2023-02-11 17:38:10,447 INFO Sample translation at epoch 6 and index 231.\n",
      "2023-02-11 17:38:10,448 INFO Predicted Statement: Nous avons eu un profit de profit -- j'ai fait un système de.\n",
      "2023-02-11 17:38:50,830 INFO Saving training state...\n",
      "2023-02-11 17:38:52,174 INFO Sample translation at epoch 6 and index 482.\n",
      "2023-02-11 17:38:52,175 INFO Predicted Statement: Nous avons eu un bénéfice. J'ai fait un jour de l'ensemble de l'architecture..\n",
      "2023-02-11 17:39:29,214 INFO Saving training state...\n",
      "2023-02-11 17:39:30,442 INFO Sample translation at epoch 6 and index 733.\n",
      "2023-02-11 17:39:30,442 INFO Predicted Statement: Nous avons eu un bénéfice. J.\n",
      "2023-02-11 17:40:10,577 INFO Saving training state...\n",
      "2023-02-11 17:40:12,022 INFO Sample translation at epoch 6 and index 984.\n",
      "2023-02-11 17:40:12,023 INFO Predicted Statement: Nous avons eu un bénéfice. J'ai fait un individu de l'.\n",
      "2023-02-11 17:40:52,010 INFO Saving training state...\n",
      "2023-02-11 17:40:53,389 INFO Sample translation at epoch 6 and index 1235.\n",
      "2023-02-11 17:40:53,389 INFO Predicted Statement: Nous avons eu un profit de profit, j'ai fait un jour d'œuvre d'un individu..\n",
      "2023-02-11 17:40:57,445 INFO Epoch 6 finished. Total loss is 1.815036073717643. Current learning rate is 0.0001\n",
      "2023-02-11 17:40:57,446 INFO Starting epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fa4934a1dd490abf144f2abda32e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:41:33,003 INFO Saving training state...\n",
      "2023-02-11 17:41:34,307 INFO Sample translation at epoch 7 and index 227.\n",
      "2023-02-11 17:41:34,308 INFO Predicted Statement: Nous avons eu un profit vicieux -- j'ai fait un rat.\n",
      "2023-02-11 17:42:16,016 INFO Saving training state...\n",
      "2023-02-11 17:42:17,437 INFO Sample translation at epoch 7 and index 478.\n",
      "2023-02-11 17:42:17,437 INFO Predicted Statement: Il y a eu un bénéfice. J'ai fait de l'ensemble de l'ensemble de l'architecture..\n",
      "2023-02-11 17:42:56,494 INFO Saving training state...\n",
      "2023-02-11 17:42:57,721 INFO Sample translation at epoch 7 and index 729.\n",
      "2023-02-11 17:42:57,722 INFO Predicted Statement: Nous avons eu un bénéfice en échange. J'ai fait un système d.\n",
      "2023-02-11 17:43:34,291 INFO Saving training state...\n",
      "2023-02-11 17:43:35,549 INFO Sample translation at epoch 7 and index 980.\n",
      "2023-02-11 17:43:35,550 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un individu d'un.\n",
      "2023-02-11 17:44:14,406 INFO Saving training state...\n",
      "2023-02-11 17:44:15,824 INFO Sample translation at epoch 7 and index 1231.\n",
      "2023-02-11 17:44:15,825 INFO Predicted Statement: Nous avons eu un profit marginal, j'ai fait fonctionner à l'œuvre..\n",
      "2023-02-11 17:44:19,990 INFO Epoch 7 finished. Total loss is 1.6349725405122668. Current learning rate is 0.0001\n",
      "2023-02-11 17:44:19,991 INFO Starting epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e589f0aa8b442b8448e1e3eee0aed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:44:55,255 INFO Saving training state...\n",
      "2023-02-11 17:44:56,737 INFO Sample translation at epoch 8 and index 223.\n",
      "2023-02-11 17:44:56,738 INFO Predicted Statement: Nous avons eu un profit marginal -- j'ai fait un système d'.\n",
      "2023-02-11 17:45:35,081 INFO Saving training state...\n",
      "2023-02-11 17:45:36,566 INFO Sample translation at epoch 8 and index 474.\n",
      "2023-02-11 17:45:36,566 INFO Predicted Statement: Nous avons eu un bénéfice marginal - j'ai fait un système d'ensembles de l'ensemble..\n",
      "2023-02-11 17:46:15,273 INFO Saving training state...\n",
      "2023-02-11 17:46:16,819 INFO Sample translation at epoch 8 and index 725.\n",
      "2023-02-11 17:46:16,820 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un système d'intelligence..\n",
      "2023-02-11 17:46:53,912 INFO Saving training state...\n",
      "2023-02-11 17:46:55,347 INFO Sample translation at epoch 8 and index 976.\n",
      "2023-02-11 17:46:55,348 INFO Predicted Statement: Nous avions un bénéfice marginal -- J'ai fait un épinement de l'un d'un individu..\n",
      "2023-02-11 17:47:35,165 INFO Saving training state...\n",
      "2023-02-11 17:47:36,696 INFO Sample translation at epoch 8 and index 1227.\n",
      "2023-02-11 17:47:36,696 INFO Predicted Statement: Nous avions un profit marginal, j'ai fait de l'un de l'a conduit à l'échelle de la vie..\n",
      "2023-02-11 17:47:41,878 INFO Epoch 8 finished. Total loss is 1.4896077902852969. Current learning rate is 0.0001\n",
      "2023-02-11 17:47:41,879 INFO Starting epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2a10f0adfc40b8b9f920ac81474b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:48:15,915 INFO Saving training state...\n",
      "2023-02-11 17:48:17,568 INFO Sample translation at epoch 9 and index 219.\n",
      "2023-02-11 17:48:17,569 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un centre de fonctionnement d'un an de l'intert.\n",
      "2023-02-11 17:48:56,845 INFO Saving training state...\n",
      "2023-02-11 17:48:58,243 INFO Sample translation at epoch 9 and index 470.\n",
      "2023-02-11 17:48:58,244 INFO Predicted Statement: Nous avons eu un bénéfice marginal -- j'ai fait de l'ensemble de l'un de l'a.\n",
      "2023-02-11 17:49:37,403 INFO Saving training state...\n",
      "2023-02-11 17:49:39,281 INFO Sample translation at epoch 9 and index 721.\n",
      "2023-02-11 17:49:39,281 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un système d'intertilna..\n",
      "2023-02-11 17:50:19,400 INFO Saving training state...\n",
      "2023-02-11 17:50:20,641 INFO Sample translation at epoch 9 and index 972.\n",
      "2023-02-11 17:50:20,641 INFO Predicted Statement: Nous avons eu un bénéfice marginal -- j'ai fait un centre d'.\n",
      "2023-02-11 17:50:58,656 INFO Saving training state...\n",
      "2023-02-11 17:50:59,975 INFO Sample translation at epoch 9 and index 1223.\n",
      "2023-02-11 17:50:59,975 INFO Predicted Statement: Nous avions un bénéfice marginal, j'ai fait un système d'interturnel..\n",
      "2023-02-11 17:51:05,444 INFO Epoch 9 finished. Total loss is 1.3695611735011397. Current learning rate is 0.0001\n",
      "2023-02-11 17:51:05,445 INFO Starting epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24c126eec3e48c698aaa02b9983a54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:51:38,260 INFO Saving training state...\n",
      "2023-02-11 17:51:39,593 INFO Sample translation at epoch 10 and index 215.\n",
      "2023-02-11 17:51:39,594 INFO Predicted Statement: Nous avions un profit marginal -- j'ai fait d'un fonctionnement de l'intertataire d'.\n",
      "2023-02-11 17:52:17,654 INFO Saving training state...\n",
      "2023-02-11 17:52:19,296 INFO Sample translation at epoch 10 and index 466.\n",
      "2023-02-11 17:52:19,297 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait de l'un de l'est..\n",
      "2023-02-11 17:52:57,687 INFO Saving training state...\n",
      "2023-02-11 17:52:58,971 INFO Sample translation at epoch 10 and index 717.\n",
      "2023-02-11 17:52:58,971 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un système d'intelligence.\n",
      "2023-02-11 17:53:36,164 INFO Saving training state...\n",
      "2023-02-11 17:53:37,884 INFO Sample translation at epoch 10 and index 968.\n",
      "2023-02-11 17:53:37,885 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait de l'un de l'autre..\n",
      "2023-02-11 17:54:14,418 INFO Saving training state...\n",
      "2023-02-11 17:54:15,741 INFO Sample translation at epoch 10 and index 1219.\n",
      "2023-02-11 17:54:15,741 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un système d'intert et d'un individu d'.\n",
      "2023-02-11 17:54:21,650 INFO Epoch 10 finished. Total loss is 1.2667469084736276. Current learning rate is 0.0001\n",
      "2023-02-11 17:54:21,651 INFO Starting epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4974060247c743dab12f700706bd89eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:54:52,286 INFO Saving training state...\n",
      "2023-02-11 17:54:53,842 INFO Sample translation at epoch 11 and index 211.\n",
      "2023-02-11 17:54:53,843 INFO Predicted Statement: Nous avions un profit marginal -- j'ai fait du fonctionnement de l'un de l'a transformé en l'allit d'un individu..\n",
      "2023-02-11 17:55:30,372 INFO Saving training state...\n",
      "2023-02-11 17:55:31,777 INFO Sample translation at epoch 11 and index 462.\n",
      "2023-02-11 17:55:31,777 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait d'un centre de l'allit..\n",
      "2023-02-11 17:56:08,083 INFO Saving training state...\n",
      "2023-02-11 17:56:09,421 INFO Sample translation at epoch 11 and index 713.\n",
      "2023-02-11 17:56:09,421 INFO Predicted Statement: Nous avons eu un bénéfice marginal -- j'ai fait un système d'intertacht..\n",
      "2023-02-11 17:56:45,652 INFO Saving training state...\n",
      "2023-02-11 17:56:47,446 INFO Sample translation at epoch 11 and index 964.\n",
      "2023-02-11 17:56:47,447 INFO Predicted Statement: Nous avons eu un bénéfice marginal -- j'ai fait un système d'amot d'amot d'un individu..\n",
      "2023-02-11 17:57:23,788 INFO Saving training state...\n",
      "2023-02-11 17:57:25,426 INFO Sample translation at epoch 11 and index 1215.\n",
      "2023-02-11 17:57:25,427 INFO Predicted Statement: Nous avions un profit marginal -- j'ai fait de l'intert et de l'anat qui.\n",
      "2023-02-11 17:57:31,873 INFO Epoch 11 finished. Total loss is 1.1777810344059758. Current learning rate is 0.0001\n",
      "2023-02-11 17:57:31,874 INFO Starting epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3203099d7d442d182aebbf35d1f9900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 17:58:01,992 INFO Saving training state...\n",
      "2023-02-11 17:58:03,394 INFO Sample translation at epoch 12 and index 207.\n",
      "2023-02-11 17:58:03,394 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un fonctionnement de l'évolution d'un individu à l'autre..\n",
      "2023-02-11 17:58:39,906 INFO Saving training state...\n",
      "2023-02-11 17:58:41,225 INFO Sample translation at epoch 12 and index 458.\n",
      "2023-02-11 17:58:41,225 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un fonctionnement de l'amotalisme d'un individu.\n",
      "2023-02-11 17:59:16,288 INFO Saving training state...\n",
      "2023-02-11 17:59:17,501 INFO Sample translation at epoch 12 and index 709.\n",
      "2023-02-11 17:59:17,502 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un système d'inter.\n",
      "2023-02-11 17:59:51,097 INFO Saving training state...\n",
      "2023-02-11 17:59:52,320 INFO Sample translation at epoch 12 and index 960.\n",
      "2023-02-11 17:59:52,321 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un système d'a.\n",
      "2023-02-11 18:00:26,331 INFO Saving training state...\n",
      "2023-02-11 18:00:27,716 INFO Sample translation at epoch 12 and index 1211.\n",
      "2023-02-11 18:00:27,716 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait fonctionner à l'anatance d'un atel..\n",
      "2023-02-11 18:00:34,327 INFO Epoch 12 finished. Total loss is 1.098497860743187. Current learning rate is 0.0001\n",
      "2023-02-11 18:00:34,329 INFO Starting epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b4f96f6bdac447bb11ff0d806c24d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:01:01,833 INFO Saving training state...\n",
      "2023-02-11 18:01:03,334 INFO Sample translation at epoch 13 and index 203.\n",
      "2023-02-11 18:01:03,334 INFO Predicted Statement: Nous avions un profit marginal -- j'ai fait de l'un de l'at-il de l'aduction..\n",
      "2023-02-11 18:01:37,310 INFO Saving training state...\n",
      "2023-02-11 18:01:38,734 INFO Sample translation at epoch 13 and index 454.\n",
      "2023-02-11 18:01:38,734 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un damot d'amotalisme qui a conduit à l'autre..\n",
      "2023-02-11 18:02:12,553 INFO Saving training state...\n",
      "2023-02-11 18:02:13,740 INFO Sample translation at epoch 13 and index 705.\n",
      "2023-02-11 18:02:13,741 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un système qui s'.\n",
      "2023-02-11 18:02:47,447 INFO Saving training state...\n",
      "2023-02-11 18:02:48,807 INFO Sample translation at epoch 13 and index 956.\n",
      "2023-02-11 18:02:48,807 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait d'un système d'amote d'autre..\n",
      "2023-02-11 18:03:22,699 INFO Saving training state...\n",
      "2023-02-11 18:03:24,117 INFO Sample translation at epoch 13 and index 1207.\n",
      "2023-02-11 18:03:24,118 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un changement d'un.\n",
      "2023-02-11 18:03:31,341 INFO Epoch 13 finished. Total loss is 1.0268416960292435. Current learning rate is 0.0001\n",
      "2023-02-11 18:03:31,342 INFO Starting epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302d22f6b4d243bba98737ffb5de7915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:03:58,334 INFO Saving training state...\n",
      "2023-02-11 18:03:59,834 INFO Sample translation at epoch 14 and index 199.\n",
      "2023-02-11 18:03:59,834 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait de l'un d'entre nous d'entre nous qui a conduit à l'est..\n",
      "2023-02-11 18:04:33,759 INFO Saving training state...\n",
      "2023-02-11 18:04:35,107 INFO Sample translation at epoch 14 and index 450.\n",
      "2023-02-11 18:04:35,107 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un dysfonctionnement d'amotalisme qui gouverne..\n",
      "2023-02-11 18:05:08,968 INFO Saving training state...\n",
      "2023-02-11 18:05:10,386 INFO Sample translation at epoch 14 and index 701.\n",
      "2023-02-11 18:05:10,386 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un stilt.\n",
      "2023-02-11 18:05:44,146 INFO Saving training state...\n",
      "2023-02-11 18:05:45,860 INFO Sample translation at epoch 14 and index 952.\n",
      "2023-02-11 18:05:45,860 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait d'un système d'amotox d'empreinte..\n",
      "2023-02-11 18:06:19,660 INFO Saving training state...\n",
      "2023-02-11 18:06:20,877 INFO Sample translation at epoch 14 and index 1203.\n",
      "2023-02-11 18:06:20,878 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un changement d'un.\n",
      "2023-02-11 18:06:28,680 INFO Epoch 14 finished. Total loss is 0.9617695295214179. Current learning rate is 0.0001\n",
      "2023-02-11 18:06:28,681 INFO Starting epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76091bb6f316410faceae7e8954e0212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:06:55,229 INFO Saving training state...\n",
      "2023-02-11 18:06:56,808 INFO Sample translation at epoch 15 and index 195.\n",
      "2023-02-11 18:06:56,808 INFO Predicted Statement: Nous avons eu un bénéfice marginal - j'ai fait de l'actest d'amotomé d'une façon qui a.\n",
      "2023-02-11 18:07:30,723 INFO Saving training state...\n",
      "2023-02-11 18:07:32,120 INFO Sample translation at epoch 15 and index 446.\n",
      "2023-02-11 18:07:32,120 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait de l'amo.\n",
      "2023-02-11 18:08:05,977 INFO Saving training state...\n",
      "2023-02-11 18:08:07,494 INFO Sample translation at epoch 15 and index 697.\n",
      "2023-02-11 18:08:07,495 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'en ai fait l'automot influe génétique..\n",
      "2023-02-11 18:08:41,149 INFO Saving training state...\n",
      "2023-02-11 18:08:42,959 INFO Sample translation at epoch 15 and index 948.\n",
      "2023-02-11 18:08:42,959 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait d'un système d'organisation et d'un système d'empreinte génétique..\n",
      "2023-02-11 18:09:16,775 INFO Saving training state...\n",
      "2023-02-11 18:09:18,883 INFO Sample translation at epoch 15 and index 1199.\n",
      "2023-02-11 18:09:18,883 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai conduit à diriger l'évolution d'un système d'auto-mote d'évolution..\n",
      "2023-02-11 18:09:27,267 INFO Epoch 15 finished. Total loss is 0.9026936279155604. Current learning rate is 0.0001\n",
      "2023-02-11 18:09:27,268 INFO Starting epoch 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8923218ca9ce41eba06f0fc12c7eac60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:09:53,258 INFO Saving training state...\n",
      "2023-02-11 18:09:54,925 INFO Sample translation at epoch 16 and index 191.\n",
      "2023-02-11 18:09:54,926 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un moteur d'inspiration pour l'atome d'intertétaché..\n",
      "2023-02-11 18:10:28,851 INFO Saving training state...\n",
      "2023-02-11 18:10:30,213 INFO Sample translation at epoch 16 and index 442.\n",
      "2023-02-11 18:10:30,214 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un moteur de l'amot neurologique..\n",
      "2023-02-11 18:11:04,056 INFO Saving training state...\n",
      "2023-02-11 18:11:05,541 INFO Sample translation at epoch 16 and index 693.\n",
      "2023-02-11 18:11:05,541 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'en ai fait un dommotom d'un pour l'autre..\n",
      "2023-02-11 18:11:39,157 INFO Saving training state...\n",
      "2023-02-11 18:11:40,522 INFO Sample translation at epoch 16 and index 944.\n",
      "2023-02-11 18:11:40,522 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un apport d'un système d'Agence d'amotomotot d'un.\n",
      "2023-02-11 18:12:14,295 INFO Saving training state...\n",
      "2023-02-11 18:12:15,656 INFO Sample translation at epoch 16 and index 1195.\n",
      "2023-02-11 18:12:15,656 INFO Predicted Statement: Nous avions un bénéfice marginal - j'en ai fait un amote spécial..\n",
      "2023-02-11 18:12:24,560 INFO Epoch 16 finished. Total loss is 0.8476522797933166. Current learning rate is 0.0001\n",
      "2023-02-11 18:12:24,562 INFO Starting epoch 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49c84be32b5410aa5387278129a3e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:12:50,002 INFO Saving training state...\n",
      "2023-02-11 18:12:51,282 INFO Sample translation at epoch 17 and index 187.\n",
      "2023-02-11 18:12:51,282 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait d'un moteur à l'autre..\n",
      "2023-02-11 18:13:25,210 INFO Saving training state...\n",
      "2023-02-11 18:13:26,529 INFO Sample translation at epoch 17 and index 438.\n",
      "2023-02-11 18:13:26,530 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un suivi de l'amotomotomoteur de.\n",
      "2023-02-11 18:14:00,369 INFO Saving training state...\n",
      "2023-02-11 18:14:01,662 INFO Sample translation at epoch 17 and index 689.\n",
      "2023-02-11 18:14:01,663 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait l'automoto diriger d'une génération d'auto.\n",
      "2023-02-11 18:14:35,296 INFO Saving training state...\n",
      "2023-02-11 18:14:36,630 INFO Sample translation at epoch 17 and index 940.\n",
      "2023-02-11 18:14:36,631 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait d'un diomte d'un diatomer..\n",
      "2023-02-11 18:15:10,606 INFO Saving training state...\n",
      "2023-02-11 18:15:12,277 INFO Sample translation at epoch 17 and index 1191.\n",
      "2023-02-11 18:15:12,278 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait a conduit à notre gouvernance..\n",
      "2023-02-11 18:15:21,633 INFO Epoch 17 finished. Total loss is 0.7965517141238577. Current learning rate is 0.0001\n",
      "2023-02-11 18:15:21,634 INFO Starting epoch 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f2259fbba614f809b7724672a73b26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:15:46,540 INFO Saving training state...\n",
      "2023-02-11 18:15:48,043 INFO Sample translation at epoch 18 and index 183.\n",
      "2023-02-11 18:15:48,043 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait d'un moteur à l'autre..\n",
      "2023-02-11 18:16:22,061 INFO Saving training state...\n",
      "2023-02-11 18:16:23,449 INFO Sample translation at epoch 18 and index 434.\n",
      "2023-02-11 18:16:23,450 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un moteur de l'amotométrie..\n",
      "2023-02-11 18:16:57,297 INFO Saving training state...\n",
      "2023-02-11 18:16:58,616 INFO Sample translation at epoch 18 and index 685.\n",
      "2023-02-11 18:16:58,616 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait d'un guide d'automote d'autre..\n",
      "2023-02-11 18:17:32,292 INFO Saving training state...\n",
      "2023-02-11 18:17:34,098 INFO Sample translation at epoch 18 and index 936.\n",
      "2023-02-11 18:17:34,098 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait d'un diatmote d'une gouvernance centrale..\n",
      "2023-02-11 18:18:07,972 INFO Saving training state...\n",
      "2023-02-11 18:18:10,197 INFO Sample translation at epoch 18 and index 1187.\n",
      "2023-02-11 18:18:10,198 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait module d'un système d'automotométrie..\n",
      "2023-02-11 18:18:20,146 INFO Epoch 18 finished. Total loss is 0.7483930458049153. Current learning rate is 0.0001\n",
      "2023-02-11 18:18:20,146 INFO Starting epoch 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70082f6443e446fcab71bdb64e48ff44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:18:44,487 INFO Saving training state...\n",
      "2023-02-11 18:18:46,706 INFO Sample translation at epoch 19 and index 179.\n",
      "2023-02-11 18:18:46,707 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait d'un moteur à l'écart de notre personnalité..\n",
      "2023-02-11 18:19:20,616 INFO Saving training state...\n",
      "2023-02-11 18:19:21,911 INFO Sample translation at epoch 19 and index 430.\n",
      "2023-02-11 18:19:21,912 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait d'un guide d'automoteur et d'.\n",
      "2023-02-11 18:19:55,723 INFO Saving training state...\n",
      "2023-02-11 18:19:57,017 INFO Sample translation at epoch 19 and index 681.\n",
      "2023-02-11 18:19:57,018 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait l'automotance d'une génération d'un.\n",
      "2023-02-11 18:20:30,822 INFO Saving training state...\n",
      "2023-02-11 18:20:32,333 INFO Sample translation at epoch 19 and index 932.\n",
      "2023-02-11 18:20:32,333 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait somnat Sante Central..\n",
      "2023-02-11 18:21:06,072 INFO Saving training state...\n",
      "2023-02-11 18:21:07,295 INFO Sample translation at epoch 19 and index 1183.\n",
      "2023-02-11 18:21:07,296 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai conduit à Centralmotom.\n",
      "2023-02-11 18:21:17,754 INFO Epoch 19 finished. Total loss is 0.7052419010351536. Current learning rate is 0.0001\n",
      "2023-02-11 18:21:17,754 INFO Starting epoch 20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b299dc332a5476e929df60710bed8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:21:41,516 INFO Saving training state...\n",
      "2023-02-11 18:21:43,395 INFO Sample translation at epoch 20 and index 175.\n",
      "2023-02-11 18:21:43,396 INFO Predicted Statement: Nous avions un bénéfice marginal - je réussis d'automoteur moteur de gouvernance et d'automotomépendé..\n",
      "2023-02-11 18:22:17,254 INFO Saving training state...\n",
      "2023-02-11 18:22:18,693 INFO Sample translation at epoch 20 and index 426.\n",
      "2023-02-11 18:22:18,694 INFO Predicted Statement: Nous avions un bénéfice marginal, je l'ai fait module d'a.\n",
      "2023-02-11 18:22:52,627 INFO Saving training state...\n",
      "2023-02-11 18:22:53,874 INFO Sample translation at epoch 20 and index 677.\n",
      "2023-02-11 18:22:53,874 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un bourgeon de l'ommotom Sanmotom.\n",
      "2023-02-11 18:23:27,669 INFO Saving training state...\n",
      "2023-02-11 18:23:29,401 INFO Sample translation at epoch 20 and index 928.\n",
      "2023-02-11 18:23:29,401 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait conduit San bourgeon Bumoteur idéal..\n",
      "2023-02-11 18:24:03,163 INFO Saving training state...\n",
      "2023-02-11 18:24:04,438 INFO Sample translation at epoch 20 and index 1179.\n",
      "2023-02-11 18:24:04,438 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait naître Sanmote Central Sanmoduction..\n",
      "2023-02-11 18:24:15,457 INFO Epoch 20 finished. Total loss is 0.6638968049462017. Current learning rate is 0.0001\n",
      "2023-02-11 18:24:15,458 INFO Starting epoch 21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a6ed0503c7437aa04fbfa39b76b327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:24:38,681 INFO Saving training state...\n",
      "2023-02-11 18:24:40,050 INFO Sample translation at epoch 21 and index 171.\n",
      "2023-02-11 18:24:40,050 INFO Predicted Statement: Nous avions un bénéfice marginal - j'ai fait un moteur d'autonat d'un moteur à caractère moteur..\n",
      "2023-02-11 18:25:13,963 INFO Saving training state...\n",
      "2023-02-11 18:25:15,353 INFO Sample translation at epoch 21 and index 422.\n",
      "2023-02-11 18:25:15,354 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait fonctionner grâce à l.\n",
      "2023-02-11 18:25:51,832 INFO Saving training state...\n",
      "2023-02-11 18:25:53,272 INFO Sample translation at epoch 21 and index 673.\n",
      "2023-02-11 18:25:53,273 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait Sante Central Imaginez notre fantastique édifice..\n",
      "2023-02-11 18:26:31,240 INFO Saving training state...\n",
      "2023-02-11 18:26:32,524 INFO Sample translation at epoch 21 and index 924.\n",
      "2023-02-11 18:26:32,525 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait idéal idéal Central Central.\n",
      "2023-02-11 18:27:10,387 INFO Saving training state...\n",
      "2023-02-11 18:27:12,121 INFO Sample translation at epoch 21 and index 1175.\n",
      "2023-02-11 18:27:12,121 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai conduit Sanmoteur idéal..\n",
      "2023-02-11 18:27:25,095 INFO Epoch 21 finished. Total loss is 0.6255560414695475. Current learning rate is 0.0001\n",
      "2023-02-11 18:27:25,096 INFO Starting epoch 22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3e312a65df94606aab16155439b7d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:27:50,212 INFO Saving training state...\n",
      "2023-02-11 18:27:52,118 INFO Sample translation at epoch 22 and index 167.\n",
      "2023-02-11 18:27:52,118 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai conduit moteur à notre gouvernance..\n",
      "2023-02-11 18:28:29,823 INFO Saving training state...\n",
      "2023-02-11 18:28:31,207 INFO Sample translation at epoch 22 and index 418.\n",
      "2023-02-11 18:28:31,207 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait de l'Agence d'automotateur d'.\n",
      "2023-02-11 18:29:09,457 INFO Saving training state...\n",
      "2023-02-11 18:29:10,828 INFO Sample translation at epoch 22 and index 669.\n",
      "2023-02-11 18:29:10,828 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai conduit l'élagage d'un bourgeon d'un produit spina.\n",
      "2023-02-11 18:29:48,257 INFO Saving training state...\n",
      "2023-02-11 18:29:49,556 INFO Sample translation at epoch 22 and index 920.\n",
      "2023-02-11 18:29:49,556 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait conduit idéal pour Central.\n",
      "2023-02-11 18:30:26,342 INFO Saving training state...\n",
      "2023-02-11 18:30:27,637 INFO Sample translation at epoch 22 and index 1171.\n",
      "2023-02-11 18:30:27,638 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai conduit Sanna Central Central.\n",
      "2023-02-11 18:30:40,873 INFO Epoch 22 finished. Total loss is 0.591315035312492. Current learning rate is 0.0001\n",
      "2023-02-11 18:30:40,874 INFO Starting epoch 23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b880a4bd280748ca96e85d911d98121d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:31:06,797 INFO Saving training state...\n",
      "2023-02-11 18:31:08,261 INFO Sample translation at epoch 23 and index 163.\n",
      "2023-02-11 18:31:08,261 INFO Predicted Statement: Nous avions un bénéfice marginal - je m'a conduit d'un moteur à l'automotode..\n",
      "2023-02-11 18:31:51,385 INFO Saving training state...\n",
      "2023-02-11 18:31:53,194 INFO Sample translation at epoch 23 and index 414.\n",
      "2023-02-11 18:31:53,195 INFO Predicted Statement: Nous avions un bénéfice marginal - je m'a conduit à diriger notre spin d'un spina moteur..\n",
      "2023-02-11 18:32:33,922 INFO Saving training state...\n",
      "2023-02-11 18:32:35,385 INFO Sample translation at epoch 23 and index 665.\n",
      "2023-02-11 18:32:35,385 INFO Predicted Statement: Nous avions un bénéfice marginal -- j'ai fait un produit d'automoteur spécial d'un réacteur à.\n",
      "2023-02-11 18:33:18,695 INFO Saving training state...\n",
      "2023-02-11 18:33:20,706 INFO Sample translation at epoch 23 and index 916.\n",
      "2023-02-11 18:33:20,706 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait gouvernance idéal. Développer notre gouvernance vamoteur idéalisé..\n",
      "2023-02-11 18:34:09,463 INFO Saving training state...\n",
      "2023-02-11 18:34:11,028 INFO Sample translation at epoch 23 and index 1167.\n",
      "2023-02-11 18:34:11,029 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai conduit San astronef San.\n",
      "2023-02-11 18:34:34,028 INFO Epoch 23 finished. Total loss is 0.5590602283301668. Current learning rate is 0.0001\n",
      "2023-02-11 18:34:34,029 INFO Starting epoch 24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e112d72ea1492ab7dc6f165d94b2b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:35:13,374 INFO Saving training state...\n",
      "2023-02-11 18:35:15,955 INFO Sample translation at epoch 24 and index 159.\n",
      "2023-02-11 18:35:15,955 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai conduit spin d'un guide spina spinum..\n",
      "2023-02-11 18:36:17,788 INFO Saving training state...\n",
      "2023-02-11 18:36:19,562 INFO Sample translation at epoch 24 and index 410.\n",
      "2023-02-11 18:36:19,562 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait module Sanmoteur Sanmotherum génétiquement..\n",
      "2023-02-11 18:37:21,113 INFO Saving training state...\n",
      "2023-02-11 18:37:23,188 INFO Sample translation at epoch 24 and index 661.\n",
      "2023-02-11 18:37:23,189 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait influe génétique artificiel..\n",
      "2023-02-11 18:38:25,238 INFO Saving training state...\n",
      "2023-02-11 18:38:26,768 INFO Sample translation at epoch 24 and index 912.\n",
      "2023-02-11 18:38:26,769 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait idéal pour Central Central.\n",
      "2023-02-11 18:39:28,969 INFO Saving training state...\n",
      "2023-02-11 18:39:30,428 INFO Sample translation at epoch 24 and index 1163.\n",
      "2023-02-11 18:39:30,428 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai conduit idéal pour Central Central.\n",
      "2023-02-11 18:39:54,799 INFO Epoch 24 finished. Total loss is 0.5289370617855154. Current learning rate is 0.0001\n",
      "2023-02-11 18:39:54,800 INFO Starting epoch 25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140365239f3a44ddbb764d3906c96f83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:40:33,255 INFO Saving training state...\n",
      "2023-02-11 18:40:34,868 INFO Sample translation at epoch 25 and index 155.\n",
      "2023-02-11 18:40:34,869 INFO Predicted Statement: Nous avions un bénéfice marginal - je l’ai fait s’est transformé d’un guide d’automo.\n",
      "2023-02-11 18:41:37,126 INFO Saving training state...\n",
      "2023-02-11 18:41:38,591 INFO Sample translation at epoch 25 and index 406.\n",
      "2023-02-11 18:41:38,591 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait catalyseur de l'.\n",
      "2023-02-11 18:42:40,779 INFO Saving training state...\n",
      "2023-02-11 18:42:43,509 INFO Sample translation at epoch 25 and index 657.\n",
      "2023-02-11 18:42:43,509 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait idéal. Développer Central Centralfield..\n",
      "2023-02-11 18:43:45,666 INFO Saving training state...\n",
      "2023-02-11 18:43:47,220 INFO Sample translation at epoch 25 and index 908.\n",
      "2023-02-11 18:43:47,221 INFO Predicted Statement: Nous avions un bénéfice marginal -- je l'ai fait fonctionner Builte.\n",
      "2023-02-11 18:44:49,598 INFO Saving training state...\n",
      "2023-02-11 18:44:51,165 INFO Sample translation at epoch 25 and index 1159.\n",
      "2023-02-11 18:44:51,166 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai conduit spin d'un.\n",
      "2023-02-11 18:45:18,792 INFO Epoch 25 finished. Total loss is 0.5011735746756824. Current learning rate is 0.0001\n",
      "2023-02-11 18:45:18,794 INFO Starting epoch 26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf28d4866dc44aaa8e568a8544b20fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:45:50,788 INFO Saving training state...\n",
      "2023-02-11 18:45:53,191 INFO Sample translation at epoch 26 and index 151.\n",
      "2023-02-11 18:45:53,191 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait fonctionner spina idéal pour diriger Centralment..\n",
      "2023-02-11 18:46:39,151 INFO Saving training state...\n",
      "2023-02-11 18:46:40,494 INFO Sample translation at epoch 26 and index 402.\n",
      "2023-02-11 18:46:40,495 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai dirigé spin d'un.\n",
      "2023-02-11 18:47:23,421 INFO Saving training state...\n",
      "2023-02-11 18:47:24,902 INFO Sample translation at epoch 26 and index 653.\n",
      "2023-02-11 18:47:24,903 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait module d'un génome Sanmotome..\n",
      "2023-02-11 18:48:08,218 INFO Saving training state...\n",
      "2023-02-11 18:48:09,587 INFO Sample translation at epoch 26 and index 904.\n",
      "2023-02-11 18:48:09,587 INFO Predicted Statement: Nous avions un bénéfice marginal - je.\n",
      "2023-02-11 18:48:50,648 INFO Saving training state...\n",
      "2023-02-11 18:48:52,363 INFO Sample translation at epoch 26 and index 1155.\n",
      "2023-02-11 18:48:52,363 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait spin d'un guide spina Central..\n",
      "2023-02-11 18:49:09,928 INFO Epoch 26 finished. Total loss is 0.4756843746063537. Current learning rate is 0.0001\n",
      "2023-02-11 18:49:09,929 INFO Starting epoch 27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a42fd540f04c728327f35553c6250a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:49:34,635 INFO Saving training state...\n",
      "2023-02-11 18:49:36,051 INFO Sample translation at epoch 27 and index 147.\n",
      "2023-02-11 18:49:36,051 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait spin d'un guide spina conduit d'un moteur.\n",
      "2023-02-11 18:50:17,857 INFO Saving training state...\n",
      "2023-02-11 18:50:19,181 INFO Sample translation at epoch 27 and index 398.\n",
      "2023-02-11 18:50:19,182 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait spin d'un.\n",
      "2023-02-11 18:51:07,709 INFO Saving training state...\n",
      "2023-02-11 18:51:09,707 INFO Sample translation at epoch 27 and index 649.\n",
      "2023-02-11 18:51:09,707 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai conduit d'un guide d'automoteur spécial..\n",
      "2023-02-11 18:52:00,121 INFO Saving training state...\n",
      "2023-02-11 18:52:01,866 INFO Sample translation at epoch 27 and index 900.\n",
      "2023-02-11 18:52:01,866 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait d'un guide d'automoteur guide d'.\n",
      "2023-02-11 18:52:53,189 INFO Saving training state...\n",
      "2023-02-11 18:52:56,047 INFO Sample translation at epoch 27 and index 1151.\n",
      "2023-02-11 18:52:56,047 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait idéal pour Central Central Central Centralmolu..\n",
      "2023-02-11 18:53:18,081 INFO Epoch 27 finished. Total loss is 0.45204375305518546. Current learning rate is 0.0001\n",
      "2023-02-11 18:53:18,082 INFO Starting epoch 28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cb93290e574f9fa99598c197592e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:53:45,526 INFO Saving training state...\n",
      "2023-02-11 18:53:46,961 INFO Sample translation at epoch 28 and index 143.\n",
      "2023-02-11 18:53:46,961 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait conduit d'un guide spina géologique..\n",
      "2023-02-11 18:54:37,216 INFO Saving training state...\n",
      "2023-02-11 18:54:38,828 INFO Sample translation at epoch 28 and index 394.\n",
      "2023-02-11 18:54:38,829 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait d'un guide.\n",
      "2023-02-11 18:55:28,522 INFO Saving training state...\n",
      "2023-02-11 18:55:30,178 INFO Sample translation at epoch 28 and index 645.\n",
      "2023-02-11 18:55:30,179 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait d'un guide.\n",
      "2023-02-11 18:56:20,476 INFO Saving training state...\n",
      "2023-02-11 18:56:22,224 INFO Sample translation at epoch 28 and index 896.\n",
      "2023-02-11 18:56:22,225 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait guide idéal pour Central Central gouvernance..\n",
      "2023-02-11 18:57:11,934 INFO Saving training state...\n",
      "2023-02-11 18:57:13,563 INFO Sample translation at epoch 28 and index 1147.\n",
      "2023-02-11 18:57:13,564 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait Central conduit spina.\n",
      "2023-02-11 18:57:37,146 INFO Epoch 28 finished. Total loss is 0.4298081014769905. Current learning rate is 0.0001\n",
      "2023-02-11 18:57:37,147 INFO Starting epoch 29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f17416ca11af46b6b4bc0aa46748e5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 18:58:05,267 INFO Saving training state...\n",
      "2023-02-11 18:58:07,012 INFO Sample translation at epoch 29 and index 139.\n",
      "2023-02-11 18:58:07,013 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait conduit d'un guide spina Central..\n",
      "2023-02-11 18:58:56,718 INFO Saving training state...\n",
      "2023-02-11 18:58:59,225 INFO Sample translation at epoch 29 and index 390.\n",
      "2023-02-11 18:58:59,225 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait de conduit d'un guide spina..\n",
      "2023-02-11 18:59:50,342 INFO Saving training state...\n",
      "2023-02-11 18:59:52,018 INFO Sample translation at epoch 29 and index 641.\n",
      "2023-02-11 18:59:52,019 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait. Développer Développ.\n",
      "2023-02-11 19:00:43,016 INFO Saving training state...\n",
      "2023-02-11 19:00:45,088 INFO Sample translation at epoch 29 and index 892.\n",
      "2023-02-11 19:00:45,088 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait artificiel d'un guide Agence géologique..\n",
      "2023-02-11 19:01:34,778 INFO Saving training state...\n",
      "2023-02-11 19:01:36,590 INFO Sample translation at epoch 29 and index 1143.\n",
      "2023-02-11 19:01:36,591 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait sommoteur idéal pour Centralmoteur..\n",
      "2023-02-11 19:02:02,583 INFO Epoch 29 finished. Total loss is 0.4101187122942816. Current learning rate is 0.0001\n",
      "2023-02-11 19:02:02,584 INFO Starting epoch 30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aafa4c12d324c6ea83fc0d78b1cdf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-11 19:02:30,717 INFO Saving training state...\n",
      "2023-02-11 19:02:32,795 INFO Sample translation at epoch 30 and index 135.\n",
      "2023-02-11 19:02:32,795 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait d'un guide damoteur spécial d'allium..\n",
      "2023-02-11 19:03:25,791 INFO Saving training state...\n",
      "2023-02-11 19:03:27,618 INFO Sample translation at epoch 30 and index 386.\n",
      "2023-02-11 19:03:27,619 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait d'un guide d'amoteur guide spécial..\n",
      "2023-02-11 19:04:19,469 INFO Saving training state...\n",
      "2023-02-11 19:04:20,973 INFO Sample translation at epoch 30 and index 637.\n",
      "2023-02-11 19:04:20,974 INFO Predicted Statement: Nous avions un bénéfice marginal - je.\n",
      "2023-02-11 19:05:13,623 INFO Saving training state...\n",
      "2023-02-11 19:05:15,482 INFO Sample translation at epoch 30 and index 888.\n",
      "2023-02-11 19:05:15,482 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait module spin d'un guide géologique..\n",
      "2023-02-11 19:06:08,832 INFO Saving training state...\n",
      "2023-02-11 19:06:10,767 INFO Sample translation at epoch 30 and index 1139.\n",
      "2023-02-11 19:06:10,768 INFO Predicted Statement: Nous avions un bénéfice marginal - je l'ai fait sommotomoteur idéal pour Centralmoteur, spina..\n",
      "2023-02-11 19:06:36,247 INFO Epoch 30 finished. Total loss is 0.3906242518528195. Current learning rate is 0.0001\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice, product\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam \n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "\n",
    "MODEL_NAME = NOTEBOOK_NAME.replace('.ipynb', '_model.pt')\n",
    "\n",
    "def save_model(model, optimizer, epoch, batch_idx):\n",
    "    training_state = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'batch_idx': batch_idx,\n",
    "    }\n",
    "    torch.save(training_state, data_filepath(f'{MODEL_NAME}.state'))\n",
    "\n",
    "def load_model(model, optimizer):\n",
    "    try:\n",
    "        training_state = torch.load(data_filepath(f'{MODEL_NAME}.state'))\n",
    "\n",
    "        model.load_state_dict(training_state['model_state_dict'])\n",
    "        optimizer.load_state_dict(training_state['optimizer_state_dict'])\n",
    "\n",
    "        return (training_state['epoch'], training_state['batch_idx'])\n",
    "    except Exception as e:\n",
    "        log.info(f\"Couldn't find a saved model. {e}\")\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize the model.\n",
    "    seq2seq = Seq2SeqTransformer(\n",
    "        EMBEDDING_DIM,\n",
    "        SOURCE_VOCAB_SIZE,\n",
    "        TARGET_VOCAB_SIZE,\n",
    "        max_len=128,\n",
    "        dropout=0.1\n",
    "    )\n",
    "    for p in seq2seq.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    seq2seq.to(DEVICE)\n",
    "\n",
    "    loss_fn = CrossEntropyLoss(ignore_index=PAD)\n",
    "    optimizer = Adam(seq2seq.parameters(), lr=0.0001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, factor=0.1, threshold=0.005, patience=0)\n",
    "\n",
    "    # Check if we have a saved model to resume training from.\n",
    "    start_epoch, start_batch_idx = load_model(seq2seq, optimizer)\n",
    "    if start_epoch is None:\n",
    "        log.info(\"Couldn't find a saved model. Starting from scratch.\")\n",
    "    else:\n",
    "        log.info(f\"Found a saved model. Starting from epoch {start_epoch} and batch index {start_batch_idx}.\")\n",
    "\n",
    "    num_batches = sum(1 for _ in data_loader())\n",
    "    save_model_every = num_batches // 5 # save the model 5 times in each echo.\n",
    "\n",
    "    log.info(\"During training, we will test with the following two sentences:\")\n",
    "    log.info(f\"Source Statement: \")\n",
    "    log.info(f\"{sp_source_model.decode(sample_enc_source_batch[0].tolist())}\")\n",
    "    log.info(f\"Target Statement: \")\n",
    "    log.info(f\"{sp_target_model.decode(sample_dec_target_batch[0].tolist())}\")\n",
    "    batch_counter = 0\n",
    "\n",
    "    try:\n",
    "        for epoch in tqdm(range(30)):\n",
    "            total_loss = 0\n",
    "            iter_count = 0\n",
    "\n",
    "            if start_epoch is not None:\n",
    "                # Resuming an interrupted training.\n",
    "                if epoch < start_epoch:\n",
    "                    continue\n",
    "                else:\n",
    "                    start_epoch = None\n",
    "\n",
    "            seq2seq.train()\n",
    "            log.info(f\"Starting epoch {epoch + 1}\")\n",
    "            for batch_idx, (enc_src, dec_src, dec_trg) in tqdm(enumerate(islice(data_loader(), num_batches)), total=num_batches):\n",
    "                # Resume training from the last index if there was an interrupted training.\n",
    "                if start_batch_idx is not None:\n",
    "                    if batch_idx < start_batch_idx:\n",
    "                        continue\n",
    "                    else:\n",
    "                        start_batch_idx = None\n",
    "\n",
    "                enc_src = tensor(enc_src).to(DEVICE)\n",
    "                dec_src = tensor(dec_src).to(DEVICE)\n",
    "                dec_trg = tensor(dec_trg).to(DEVICE)\n",
    "\n",
    "                # Use the model to make predictions.\n",
    "                # Notice that for the target sentence, we pass the expected statement from the beginning\n",
    "                # up until the word before the last, and expect the model to predict the sentence from\n",
    "                # the first word onwards.\n",
    "                predictions = seq2seq(enc_src, dec_src)\n",
    "\n",
    "                # Calculate loss and update the parameters.\n",
    "                optimizer.zero_grad()\n",
    "                loss = loss_fn(\n",
    "                    predictions.reshape(-1, predictions.shape[-1]),\n",
    "                    dec_trg.reshape(-1)\n",
    "                )\n",
    "                loss.backward()\n",
    "                total_loss += loss.item()\n",
    "                iter_count += 1\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(seq2seq.parameters(), max_norm=1)\n",
    "                \n",
    "                optimizer.step()\n",
    "\n",
    "                # See whether it is time to save.\n",
    "                batch_counter += 1\n",
    "                if batch_counter >= save_model_every:\n",
    "                    log.info('Saving training state...')\n",
    "                    save_model(seq2seq, optimizer, epoch, batch_idx)\n",
    "                    batch_counter = 0\n",
    "                    \n",
    "                    # Try the model.\n",
    "                    seq2seq.eval()\n",
    "\n",
    "                    # Try the model after training.\n",
    "                    translation = translate_sentence_beam(\n",
    "                        seq2seq,\n",
    "                        tensor([sample_enc_source_batch[0]]).to(DEVICE),\n",
    "                        sp_target_model,\n",
    "                        # we can use dec_src or dec_trg here, they are of the same length, just\n",
    "                        # dec_src has BOS at the beginning and dec_trg has EOS at the end.\n",
    "                        max_output_len=dec_trg.shape[1]\n",
    "                    )\n",
    "\n",
    "                    log.info(f\"Sample translation at epoch {epoch+1} and index {batch_idx+1}.\")\n",
    "                    log.info(f\"Predicted Statement: {translation}.\")\n",
    "\n",
    "                    seq2seq.train()\n",
    "            \n",
    "            avg_loss = total_loss / iter_count\n",
    "            scheduler.step(avg_loss)\n",
    "            log.info(f\"Epoch {epoch + 1} finished. Total loss is {avg_loss}. Current learning rate is {optimizer.param_groups[0]['lr']}\")\n",
    "                        \n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        log.info(\"Training stopped because the user pressed Ctrl-C. Returning the model to the state it was just before interruption.\")\n",
    "        pass\n",
    "\n",
    "\n",
    "    return seq2seq, optimizer\n",
    "\n",
    "seq2seq, optimizer = train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment es-tu ?\n"
     ]
    }
   ],
   "source": [
    "from torch import tensor\n",
    "\n",
    "for sentence in [\n",
    "    'How are you?',\n",
    "]:\n",
    "    t = tensor([sp_source_model.encode(sentence) + [EOS]])\n",
    "    #t = tensor([sample_enc_source_batch[0].tolist()])\n",
    "    seq2seq.eval()\n",
    "    trans = translate_sentence_beam(seq2seq,\n",
    "        t.to(DEVICE),\n",
    "        sp_target_model,\n",
    "        MAX_SENTENCE,\n",
    "        4\n",
    "    )\n",
    "    print(trans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e224ac6763cfc7e9fb861998ea2e9606858bafea6e04ec4114991043fb8cf7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
